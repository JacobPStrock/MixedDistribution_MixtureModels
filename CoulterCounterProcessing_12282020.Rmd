---
title: "Coulter Counter Processing Mixtures"
author: "Jacob Strock"
date: "12/11/2020"
output:
  html_document:
    keep_md: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
In this vignette I will show you how to process coulter counter data with a mixture model. 

*What is a mixture model?*
Well, intuitively it is exactly what we expect the data to be, a mixture of distributions. In terms of our coulter counter, we know different species and particle types will appear in the size frequency distribution of the data. By using a mixture model we will characterize the distribution of each of these particle types, as well as learn the weights of each distribution. If you multiply these weights by the total counts of particles, voila, we have the count of particles belonging to each of these distribution. In words, this is what we get applying mixture models to these data. A mixture model will have the following general form, where :

$P(x)=\Sigma_{i=1}^{k}\pi_k f(x|\theta_k)$

The mixture model becomes especially important when the distributions may not be easily seperable. Manual gating and discrete seperations such as by k-means are going to be unrealistic here. We want a model that incorporates the fact that these particles distributions are mixed together. 

There are several important features we are going to have to account for in the coulter counter data. First we are dealing with particle size. This means that all our data must be positive. Second, we know that the distributions in the data can take very different shapes: some highly skewed, some symmetric, different locations, different scales. It is also commonly noted that detrital material often has an exponential signal in the size frequency data: that is there are many many small particles and this signal exponentially decays. How can we accomodate this? A mixture of gammas. 

Let's speak now with a little more math. The gamma distribution with shape and scale parameterization is defined as:
XXXX
This distribution is always positive, can be symmetric when XXX, and can be skewed when XXX. In fact, when XXX is XXX, we have a special case of the gamma distribution, which is the exponential distribution. This last fact can be seen below:
XXX

So, if we use a mixture of gammas we can incorporate those important features like the exponential decay of detrital material, as well as all the actual populations of plankton in the data. Mathamatically, the distribution of this model has the form:
XXXX

We will solve for our unknown parameters XXX, XXX, XXX, via what is called the EM algorithm, that is Expectation, Maximization. It is two step: 
1.) We find the Expected likelihood of our parameters over some latent parameters XXX for convenience
2.) We find the Maximum likelihood estimates of our other unknown parameters XXX from the expected likelihood.

Repeat until convergence. 

For the gamma mixture model, in the E step we find XXX as XXX:

In the maximization step, because we cannot find the maxima directly as we can't solve for the roots analytically, we will use the Newton Raphson algorithm to solve for the roots of alpha. 
XXXX

Given alpha, we can find the roots of beta directly. 
XXXX

Sounds like a beautiful model, right? Let's try it out with some data!

Load a data file and have a looksie.
```{r, message=FALSE}
setwd("C:/Users/Jacob Strock/Documents/Menden-Deuer Lab/Misc/")
df=read.csv('gyro_2_test2_rep4.csv')
plot(df$Bin,df$Count,type='l',ylab="Count", xlab="Bin")
```

The data file is frequency data. Let's convert the frequency data into raw observations
```{r, message=FALSE}
library(vcdExtra)
y = expand.dft(df,freq = "Count")
yn= na.omit(as.vector(as.numeric(as.character(y[,1]))))
hist(yn, breaks = 50)
```

Subsample the data for speed, but replot the histogram to make sure this sample size will adequately describe the data.
```{r}
RS=sample(1:length(yn),5000)
YS=yn[RS]
hist(YS, breaks=50)
```

Specify the number of expected groups in the data (including the exponential cline of detrital material).
```{r}
g=5
```

EM algorithm: Iteratively finding the expected cluster ID for each particle, then find the maximum likelihood estimates of the parameter values for each of the clusters. Store all of this. For this data we are using a mixture of an exponential distribution for the detrital material, and gamma distributions for the populations of interest in the data. We will manually specify the number of populations because in this lab data, we know how many should be there. Of course you can add more if you see interesting features in the data you want to isolate.

Mathematical details of the two cluster case can be found in  EriÅoÄlu et al. 2011. Here, I have added the flexibility of more clusters.

EriÅoÄlu, Ã., EriÅoÄlu, M., & Erol, H. (2011). A mixture model of two different distributions approach to the analysis of heterogeneous survival data. International Journal of Computational and Mathematical Sciences, 5(2), 75-79.


If you are adding or removing clusters, you will need to add or remove new shape, and scale parameters for the gamma distributions. A quick initialization that works well here is to provide an $\alpha$ value at the mean of the peaks. A $\beta of 1 will work fine. Note, the mean of a gamma distribution is $\alpha\beta$ and the variance is $\alpha\beta^2$. If you are having trouble with fits, try adjusting these starting parameters closer to the expected moments, and or give a greater number of EM iterations to reach convergence.
```{r}
y=YS
#number of EM iterations
r = 500

#distribution weights
pi.out     = matrix(0, nrow = r, ncol = (g+1))
lambda.out = rep(0,r+1)
alpha.out  = matrix(0, nrow = r, ncol = g)
beta.out   = matrix(0, nrow = r, ncol = g)
p.group    = matrix(0, nrow = length(y), ncol = (g))
w.group    = matrix(0, nrow = length(y), ncol = (g))

#initialize
pi.out[1,]    = 1/(g)    #equal weights per cluster
alpha.out[1,] = c(20, 30, 40, 60, 100)#starting alpha for each group can change, not super important because will converge
beta.out[1,]  = c(1, 1, 1, 1, 1)#starting alpha for each group can change, not super important because will converge

#Run EM
for(i in 1:(r)){
  #Expectation step (expected group ID)
  for(gg in 1:g){
    p.group[,gg]=pi.out[i,gg]*dgamma(y, shape = alpha.out[i,gg], scale = beta.out[i,gg])
  }
  for(gg in 1:(gg+1)){
    w.group[,gg] = p.group[,gg]/apply(p.group,1,sum) #expected group assignment for each observation
  }
  
  #Maximization step
  pi.out[i+1,]    = apply(w.group,2,sum)/length(y) #updated weights
  
  for(gg in 1:g){
    
    #Newton Raphson for alpha
    alpha.r = alpha.out[i,gg]
    Dalpha.relative = 1
    while(Dalpha.relative>0.01){
      a.n =  log(alpha.r)-
      digamma(alpha.r)-
      log(sum(w.group[,gg]*y)/(pi.out[i+1,gg]*length(y)))+
      sum(w.group[,gg]*log(y))/(pi.out[i+1,gg]*length(y))
      
      a.d =  1/alpha.r-trigamma(alpha.r)
      alpha.r2 = alpha.r-a.n/a.d
      alpha.r2 = ifelse(alpha.r2<0,0.01,alpha.r2)
      Dalpha.relative = abs((alpha.r-alpha.r2)/alpha.r)
      alpha.r  = alpha.r2
    }
    
    alpha.out[i+1,gg] = alpha.r
    
    #Beta
    beta.out[i+1,gg]  = sum(w.group[,1+gg]*y) / (alpha.out[i+1,gg]*pi.out[i+1,1+gg]*length(y))
  }
}
```

Make a grid of points to check the density.
```{r}
x.grid = seq(1,max(y), by=0.1)
x      = rep(x.grid, g)
y.pred = 0
ID     = 0
N      = length(y)
Pi     = pi.out[r,]
for(gg in 1:(g)){
  y.pred=c(y.pred,dgamma(x.grid,shape=alpha.out[r,gg],scale = beta.out[r,gg])*N*Pi[gg])
  ID = c(ID,rep(paste0("Cluster",gg),length(x.grid)))
}
pp = as.data.frame(do.call(cbind, list(ID[-1],y.pred[-1],x)))
```

Lets plot those densities with the data and see if it makes sense. Add/remove clusters as needed.
```{r, message=FALSE}
library(ggplot2)

YS=as.data.frame(y)
colnames(YS)='Raw'
pp$V2=as.numeric(as.character(pp$V2))
pp$V3=as.numeric(as.character(pp$V3))

ggplot(YS,aes(x=Raw))+
  geom_histogram(aes(y=..density..),fill='white',color='black')+
  stat_bin(bins=100,fill='white',color='black')+
  geom_line(data = pp, aes(x=V3, y=V2, col=V1),lwd=1.1)+
  scale_color_manual(values=rainbow(g+1),name='Cluster')+
  xlab('bin')+
  ylab('Fequency/Density')

```

If we want to know how many particles belong to each of these distributions, it is just the converged value of the weight for that distribution multiplied by the total particle count.
```{r, message=FALSE}
library(formattable)
y = expand.dft(df,freq = "Count")
yn= na.omit(as.vector(as.numeric(as.character(y[,1]))))
ID=unique(ID)[-1]
Count=round(Pi*length(yn),digits = 0)
Out.table=as.data.frame(cbind(ID,Count))
formattable(Out.table)
```

